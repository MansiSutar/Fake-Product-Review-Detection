{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4FE2sKoIvTJu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mnssu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mnssu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mnssu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask,request, render_template\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "from nltk.corpus import words\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "Fg6ZQqXgx7jG",
    "outputId": "268cc414-5244-4c99-8ea5-8ab7b06223c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jun/2022 09:00:32] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022C1015E9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022C0EC89280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jun/2022 09:00:37] \"POST /result HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jun/2022 09:35:53] \"POST /result HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "ANN = tf.keras.models.load_model('ann.h5', compile = False)\n",
    "RNN = tf.keras.models.load_model('rnn.h5', compile = False)\n",
    "DRNN = tf.keras.models.load_model('drnn.h5', compile = False)\n",
    "\n",
    "\n",
    "app = Flask(__name__, template_folder='templates')  #flask application\n",
    "\n",
    "def decontracted(phrase):    # This function decontract words like it's to it is.    \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def cleanPunctuations(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "        \n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "\n",
    "@app.route('/result', methods = ['GET', 'POST'])\n",
    "def result():\n",
    "    if request.method == 'POST':\n",
    "        review = request.form['review']\n",
    "        word = sum([len(title.split()) for title in review])\n",
    "        upper= sum([sum(char.isupper() for char in title) for title in review])\n",
    "        special = sum([sum(char in string.punctuation for char in title) for title in review]   )\n",
    "        Uniquewords = len(set(review.split()))\n",
    "        sa = SentimentIntensityAnalyzer()\n",
    "        compound = sa.polarity_scores(review)['compound']\n",
    "        if compound > 0:\n",
    "            sentiment=1\n",
    "        else:\n",
    "            sentiment=0\n",
    "        \n",
    "        i=0\n",
    "        str1=' '\n",
    "        final_string=[]\n",
    "        s=''\n",
    "        stop = set(stopwords.words('english')) #set of stopwords\n",
    "        sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "        filtered_sentence=[]\n",
    "        sent = decontracted(review)\n",
    "        for w in sent.split():\n",
    "            for cleaned_words in cleanPunctuations(w).split():\n",
    "                if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                    if(cleaned_words.lower() not in stop):\n",
    "                        s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                        filtered_sentence.append(s)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "        str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "        final_string.append(str1)\n",
    "        i+=1\n",
    "            \n",
    "        tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df = 0.5, max_df = 1.0, max_features=2) #in scikit-learn\n",
    "        tf_idf_vect.fit(final_string)    #fitting vectorizer on train data\n",
    "        final_review = tf_idf_vect.transform(final_string).toarray()\n",
    " \n",
    "        pred_ann = ANN.predict(final_review)\n",
    "        pred_rnn = RNN.predict(final_review)\n",
    "        pred_drnn = DRNN.predict(final_review)\n",
    "        \n",
    "        resultSet = pd.DataFrame()\n",
    "        resultSet=resultSet.append([[ \"ANN\", pred_ann]])\n",
    "        resultSet=resultSet.append([[\"RNN\", pred_rnn]])\n",
    "        resultSet=resultSet.append([[\"DRNN\", pred_drnn]])\n",
    "        resultSet.columns=[\"Models\",'Predictions']\n",
    "    \n",
    "        return render_template(\"result.html\", tables = [resultSet.to_html(classes='data')],tablestitle = resultSet.columns.values)\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "flask.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "0f872e417ba89275898b65d82b92293ef77e5ec4a5804fb2660bff1e9589eb69"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
